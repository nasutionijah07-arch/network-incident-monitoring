{
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "lastEditStatus": {
   "notebookId": "hgtfhkjzhsgyk22wtrqf",
   "authorId": "354714690298",
   "authorName": "NOCOPERATOR",
   "authorEmail": "jihad.akbar@berca.co.id",
   "sessionId": "0a2eb74e-8c9d-4717-b0b6-1998a094cccf",
   "lastEditTime": 1763308980035
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70031fab-33ad-4ebd-8368-d3d55aac9bf2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Snowpark_Session"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e0afd-b6c4-4004-b78b-b50832d13406",
   "metadata": {
    "language": "python",
    "name": "Library"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json, os\n",
    "from collections import Counter, defaultdict, deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, roc_curve, brier_score_loss,\n",
    "    confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from snowflake.ml.registry import Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa575a9-8416-4cd8-8eff-74145e2b222a",
   "metadata": {
    "language": "python",
    "name": "Option"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fd236-39ab-45b1-94b2-c103737ecfb3",
   "metadata": {
    "language": "python",
    "name": "Read_to_pandas"
   },
   "outputs": [],
   "source": "xf = session.table(\"HACKATHON.DATAMART.FACT_ML_FEATURES\").to_pandas()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa84038-a470-44ab-9f36-389252acd122",
   "metadata": {
    "language": "python",
    "name": "Re_run"
   },
   "outputs": [],
   "source": [
    "df = xf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c132e-45b0-400c-a0ff-6663a9b74165",
   "metadata": {
    "language": "python",
    "name": "Convert_to_Lowercase"
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdbab2-28ab-4123-9771-cf9d1aefec02",
   "metadata": {
    "language": "python",
    "name": "Explore_Data"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaed7d-218d-4289-851f-e2f1181dda81",
   "metadata": {
    "language": "python",
    "name": "Explore_Data_2"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3af46a-961a-435f-a064-c47ce222e2b7",
   "metadata": {
    "language": "python",
    "name": "Parse_Time_n_Sort_Time"
   },
   "outputs": [],
   "source": [
    "df = df.sort_values('timestamp_1h').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c2e1c-8bc1-4242-8988-ae4a7936b0c8",
   "metadata": {
    "language": "python",
    "name": "Sorting_Time"
   },
   "outputs": [],
   "source": [
    "# Plot after sorting\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df['timestamp_1h'])\n",
    "plt.title('After Sorting')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('timestamp_1h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae65e57-7e94-414d-838d-003814e5cf94",
   "metadata": {
    "language": "python",
    "name": "Sort_by_Row_Number"
   },
   "outputs": [],
   "source": [
    "# Sort by the column 'ROW_NUMBER' and reset the index\n",
    "df = df.sort_values(by='row_number').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16489dc2-7ae4-4664-8c03-d3784bdb6a89",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Explore"
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d0ae6-9ddf-48cf-9c0e-99ca6a8ea001",
   "metadata": {
    "language": "python",
    "name": "Explore_2"
   },
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd99a73-568a-4d87-b45c-d93b1b733b0d",
   "metadata": {
    "language": "python",
    "name": "Splitting_dataset"
   },
   "outputs": [],
   "source": [
    "# Dataset split:\n",
    "# - 20% for test\n",
    "# - 10% of the remaining data for validation\n",
    "# - 5% of the training data for calibration (make probability predictions more accurate)\n",
    "test_frac = 0.20\n",
    "val_frac  = 0.10\n",
    "cal_frac  = 0.05\n",
    "\n",
    "n = len(df)\n",
    "test_cut = int(n * (1 - test_frac))\n",
    "pre_test = df.iloc[:test_cut].copy()\n",
    "test_df  = df.iloc[test_cut:].copy()\n",
    "\n",
    "val_cut  = int(len(pre_test) * (1 - val_frac))\n",
    "train_df = pre_test.iloc[:val_cut].copy()\n",
    "val_df   = pre_test.iloc[val_cut:].copy()\n",
    "\n",
    "cal_cut  = int(len(train_df) * (1 - cal_frac))\n",
    "train_core = train_df.iloc[:cal_cut].copy()\n",
    "cal_df     = train_df.iloc[cal_cut:].copy()\n",
    "\n",
    "for name, df_ in [(\"train_core\", train_core), (\"cal\", cal_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    print(name, df_['timestamp_1h'].min(), \"→\", df_['timestamp_1h'].max(), \"rows:\", len(df_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084d161-e0fc-4c82-9b58-b5b48139735c",
   "metadata": {
    "language": "python",
    "name": "Percentage_Target_Variable"
   },
   "outputs": [],
   "source": [
    "def show_target(df, name):\n",
    "    p = df['label_outage_1h'].mean()*100\n",
    "    print(f\"{name:<10} pos%={p:.4f}  (pos={df['label_outage_1h'].sum():,}, n={len(df):,})\")\n",
    "\n",
    "show_target(train_core, \"train_core\")\n",
    "show_target(cal_df,     \"cal\")\n",
    "show_target(val_df,     \"val\")\n",
    "show_target(test_df,    \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b8e0d-dc20-48d9-b3b7-dbd31b284be5",
   "metadata": {
    "language": "python",
    "name": "Handling_outliers"
   },
   "outputs": [],
   "source": [
    "# Numeric non-binary\n",
    "selected_feats = [\n",
    "    'ont_registered',\n",
    "    'offline_ont_now',\n",
    "    'offline_ont_ratio',\n",
    "    'link_loss_count',\n",
    "    'bad_rsl_count',\n",
    "    'high_temp_count',\n",
    "    'dying_gasp_count',\n",
    "    'trap_trend_score',\n",
    "    'fault_rate',\n",
    "    'snr_avg',\n",
    "    'rx_power_avg',\n",
    "    'rx_power_avg_dbm',\n",
    "    'temperature_avg_c',\n",
    "    'temp_anomaly_score',\n",
    "    'hour_of_day',\n",
    "    'day_of_week'\n",
    "]\n",
    "\n",
    "# Initialize all with None\n",
    "capping_limits = {feature: None for feature in selected_feats}\n",
    "\n",
    "capping_limits['ont_registered'] = [0, 100]\n",
    "capping_limits['offline_ont_now'] = [0, 100] # extreme outliers, count\n",
    "capping_limits['offline_ont_ratio'] = [0, 98] # extreme outliers\n",
    "capping_limits['link_loss_count'] = [0, 100] # extreme outliers, count\n",
    "capping_limits['bad_rsl_count'] = [0, 100] # extreme outliers, count\n",
    "capping_limits['high_temp_count'] = [0, 100] # extreme outliers, count\n",
    "capping_limits['dying_gasp_count'] = [0, 100] # extreme outliers, count\n",
    "capping_limits['trap_trend_score'] = [1, 98] # extreme outliers\n",
    "capping_limits['fault_rate'] = [0, 99.5] # extreme outliers\n",
    "capping_limits['snr_avg'] = [0.5, 100] # extreme outliers\n",
    "capping_limits['rx_power_avg'] = [0.15, 100] # extreme outliers\n",
    "capping_limits['rx_power_avg_dbm'] = [0.15, 100] # extreme outliers\n",
    "capping_limits['temperature_avg_c'] = [0.5, 99.5] # extreme outliers\n",
    "capping_limits['temp_anomaly_score'] = [0.5, 99.5] # extreme outliers\n",
    "capping_limits['hour_of_day'] = [0, 100]\n",
    "capping_limits['day_of_week'] = [0, 100]\n",
    "\n",
    "def cap_values(series, lower, upper):\n",
    "    return np.clip(series, lower, upper)\n",
    "\n",
    "caps = {}\n",
    "\n",
    "for col, (low_p, high_p) in capping_limits.items():\n",
    "    # Compute caps from TRAINING data only\n",
    "    lower_cap = -np.inf if low_p == 0 else np.percentile(train_core[col], low_p)\n",
    "    upper_cap = np.inf if high_p == 100 else np.percentile(train_core[col], high_p)\n",
    "    caps[col] = (lower_cap, upper_cap)\n",
    "\n",
    "    before_min, before_max = train_core[col].min(), train_core[col].max()\n",
    "    num_low = (train_core[col] < lower_cap).sum()\n",
    "    num_high = (train_core[col] > upper_cap).sum()\n",
    "\n",
    "    print(f\"\\n[{col}]\")\n",
    "    print(f\"  Before: min={before_min:.4f}, max={before_max:.4f}\")\n",
    "    print(f\"  Caps: lower={lower_cap:.4f}, upper={upper_cap:.4f}\")\n",
    "    print(f\"  Rows capped: low={num_low}, high={num_high}\")\n",
    "\n",
    "    for df in [train_core, cal_df, val_df, test_df]:\n",
    "        df[col] = cap_values(df[col], lower_cap, upper_cap)\n",
    "\n",
    "    after_min, after_max = train_core[col].min(), train_core[col].max()\n",
    "    print(f\"  After:  min={after_min:.4f}, max={after_max:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbaf63d-c3c9-43f6-830c-0a57f320dca2",
   "metadata": {
    "language": "python",
    "name": "Handling_Skewness"
   },
   "outputs": [],
   "source": [
    "# The skewed count-like features\n",
    "skewed_feats = ['offline_ont_now', 'bad_rsl_count', 'link_loss_count', 'dying_gasp_count', 'high_temp_count']\n",
    "\n",
    "# Apply log1p transform safely (no drop)\n",
    "for df in [train_core, cal_df, val_df, test_df]:\n",
    "    for col in skewed_feats:\n",
    "        # Add small epsilon to avoid all zeros\n",
    "        clipped = np.clip(df[col], 0, None)\n",
    "        df[f'{col}_log'] = np.log1p(clipped + 1e-10)  # Small epsilon\n",
    "\n",
    "# Build list of log-transformed feature names\n",
    "skewed_feats_log = [f'{col}_log' for col in skewed_feats]\n",
    "\n",
    "# Compute skew only for non-binary numeric columns (selected + log-transformed)\n",
    "skewed = train_core[selected_feats + skewed_feats_log].skew().sort_values(ascending=True)\n",
    "\n",
    "# Filter skewed Series to exclude features in skewed_feats\n",
    "skewed_filtered = skewed[~skewed.index.isin(skewed_feats)]\n",
    "\n",
    "# Print like a pandas Series\n",
    "for feature, skew_value in skewed_filtered.items():\n",
    "    print(f\"{feature:<20} {skew_value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0736f-0ecd-4e40-b1bd-30c72f0a5b79",
   "metadata": {
    "language": "python",
    "name": "Dropping_Multicollinearity_Cols"
   },
   "outputs": [],
   "source": [
    "# Drop them manually from each dataframe\n",
    "for df in [train_core, cal_df, val_df, test_df]:\n",
    "    df.drop(columns=skewed_feats, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f7604-d39d-4687-b43b-eb957e579988",
   "metadata": {
    "language": "python",
    "name": "Target_Distribution"
   },
   "outputs": [],
   "source": [
    "def target_dist(df, col_target, target_1_label='Positive', target_0_label='Negative'):\n",
    "    # Smaller global font size\n",
    "    mpl.rcParams['font.size'] = 8\n",
    "\n",
    "    # Count values\n",
    "    r = df[col_target].value_counts().sort_index()\n",
    "    total = r.sum()\n",
    "\n",
    "    # Labels in correct order (0 → Negative, 1 → Positive)\n",
    "    labels = [target_0_label if i == 0 else target_1_label for i in r.index]\n",
    "    \n",
    "    # Custom function for % and count\n",
    "    def make_autopct(values):\n",
    "        def my_autopct(pct):\n",
    "            val = int(round(pct * total / 100.0))\n",
    "            return f\"{pct:.1f}%\\n({val:,})\"\n",
    "        return my_autopct\n",
    "\n",
    "    # Smaller figure and radius\n",
    "    fig, ax = plt.subplots(figsize=(2.2, 2.2))  # compact figure\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        r,\n",
    "        explode=[0.02, 0.04],\n",
    "        labels=labels,\n",
    "        radius=0.9,\n",
    "        autopct=make_autopct(r),\n",
    "        shadow=False,\n",
    "        startangle=45,\n",
    "        colors=['#66b3ff', '#ff9999'],\n",
    "        textprops={'color': 'black', 'fontsize': 7}\n",
    "    )\n",
    "\n",
    "    # Clean styling\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_frame_on(False)\n",
    "    plt.setp(autotexts, size=6, weight=\"bold\", color=\"white\")\n",
    "\n",
    "    # Compact title and layout\n",
    "    plt.title(f\"{col_target} Distribution\", fontweight=\"bold\", fontsize=9, pad=6)\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "target_dist(train_core, 'label_outage_1h', target_1_label='Outage', target_0_label='No Outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df40e41-7b23-4b17-98e2-127f8855268a",
   "metadata": {
    "language": "python",
    "name": "Feature_Engineering"
   },
   "outputs": [],
   "source": [
    "GROUP_KEY = 'olt_id'\n",
    "ROLL_KEYS = [\n",
    "    'link_loss_count_log','bad_rsl_count_log','high_temp_count_log',\n",
    "    'dying_gasp_count_log','offline_ont_now_log'\n",
    "]\n",
    "\n",
    "def _safe_numeric(df, cols):\n",
    "    f = df.copy()\n",
    "    for c in cols:\n",
    "        if c in f.columns:\n",
    "            f[c] = pd.to_numeric(f[c], errors='coerce')\n",
    "    # Replace inf/-inf from logs, then fill NaN later\n",
    "    f.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return f\n",
    "\n",
    "def add_time_feats(f):\n",
    "    f = f.copy()\n",
    "    f['hour_sin'] = np.sin(2*np.pi * f['hour_of_day']/24.0)\n",
    "    f['hour_cos'] = np.cos(2*np.pi * f['hour_of_day']/24.0)\n",
    "    return f\n",
    "\n",
    "def add_roll_delta(f, group_key=GROUP_KEY, windows=(6, 24)):\n",
    "    f = _safe_numeric(f, ROLL_KEYS)\n",
    "    f = f.sort_values(['timestamp_1h', group_key]).copy()\n",
    "    # Deltas on LOG-space (interpretable as multiplicative change)\n",
    "    for col in ROLL_KEYS:\n",
    "        if col in f.columns:\n",
    "            f[col + \"_delta_1h\"] = f.groupby(group_key)[col].diff().astype(float)\n",
    "    # Rolling means on LOG-space (smooths multiplicative noise)\n",
    "    for w in windows:\n",
    "        for col in ROLL_KEYS:\n",
    "            if col in f.columns:\n",
    "                f[f\"{col}_roll{w}h_mean\"] = (\n",
    "                    f.groupby(group_key)[col]\n",
    "                     .rolling(w, min_periods=1).mean()\n",
    "                     .reset_index(level=0, drop=True)\n",
    "                     .astype(float)\n",
    "                )\n",
    "    return f\n",
    "\n",
    "def prepare_block(df):\n",
    "    f = add_time_feats(df)\n",
    "    # ensure *_log present; if not, warn minimally\n",
    "    missing_logs = [c for c in ROLL_KEYS if c not in f.columns]\n",
    "    if missing_logs:\n",
    "        print(\"WARNING: missing log columns:\", missing_logs)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305b3b5-b290-4b7c-bbd1-80ba8f273682",
   "metadata": {
    "language": "python",
    "name": "Add_Features"
   },
   "outputs": [],
   "source": [
    "# Prepare blocks\n",
    "train_core_f = prepare_block(train_core)\n",
    "cal_f        = prepare_block(cal_df)\n",
    "val_f        = prepare_block(val_df)\n",
    "test_f       = prepare_block(test_df)\n",
    "\n",
    "# Compute roll/delta chronologically across all to avoid leakage\n",
    "concat_all = pd.concat([train_core_f, cal_f, val_f, test_f], axis=0).sort_values('timestamp_1h')\n",
    "concat_all = add_roll_delta(concat_all, group_key=GROUP_KEY, windows=(6,24))\n",
    "\n",
    "train_core_f = concat_all.loc[train_core_f.index].copy()\n",
    "cal_f        = concat_all.loc[cal_f.index].copy()\n",
    "val_f        = concat_all.loc[val_f.index].copy()\n",
    "test_f       = concat_all.loc[test_f.index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62ae93d-03e4-44c1-bc8e-6732176ee6df",
   "metadata": {
    "language": "python",
    "name": "Features"
   },
   "outputs": [],
   "source": [
    "# Base numeric (no raw IDs)\n",
    "num_base = [\n",
    "    'offline_ont_ratio','trap_trend_score','fault_rate',\n",
    "    'snr_avg','rx_power_avg_dbm','temperature_avg_c','temp_anomaly_score',\n",
    "    'hour_sin','hour_cos','is_maintenance_window'\n",
    "]\n",
    "\n",
    "# Build feature list: *_log deltas/rollings + base\n",
    "roll_cols = [c for c in train_core_f.columns if any(s in c for s in [\n",
    "    \"_delta_1h\",\"_roll6h_mean\",\"_roll24h_mean\"\n",
    "]) and any(c.startswith(k) for k in ROLL_KEYS)]\n",
    "\n",
    "feature_cols_all = [c for c in num_base if c in train_core_f.columns] + roll_cols\n",
    "\n",
    "# Clean NaNs from log ops later during matrix build\n",
    "target_col = 'label_outage_1h'\n",
    "print(\"Num features:\", len(feature_cols_all))\n",
    "print(sorted(feature_cols_all)[:12], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405fdb5-3cbe-4f2b-b711-b063afbdfbbb",
   "metadata": {
    "language": "python",
    "name": "Multicollinearity_Issue"
   },
   "outputs": [],
   "source": [
    "col_to_drop = ['temp_anomaly_score']\n",
    "# Drop the column in-place for each DataFrame\n",
    "for df in [train_core_f, cal_f, val_f, test_f]:\n",
    "    df.drop(columns=col_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e8d979-4ee7-4dee-b6b3-3694fd8a157d",
   "metadata": {
    "language": "python",
    "name": "Update_All_Cols_Features"
   },
   "outputs": [],
   "source": [
    "feature_cols_all = [col for col in feature_cols_all if col not in col_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e9dc1-9b1b-49f3-ac33-246f429c718d",
   "metadata": {
    "language": "python",
    "name": "Splitting_Vars"
   },
   "outputs": [],
   "source": [
    "def make_xy(frame, feats, target):\n",
    "    X = frame[feats].copy()\n",
    "    # Replace NaNs introduced by logs/diffs/rolling\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0).astype(float)\n",
    "    y = frame[target].astype(int)\n",
    "    return X, y\n",
    "\n",
    "X_tr, y_tr = make_xy(train_core_f, feature_cols_all, target_col)\n",
    "X_cal, y_cal = make_xy(cal_f, feature_cols_all, target_col)\n",
    "X_va,  y_va  = make_xy(val_f,  feature_cols_all, target_col)\n",
    "X_te,  y_te  = make_xy(test_f, feature_cols_all, target_col)\n",
    "\n",
    "print(\"Shapes:\", X_tr.shape, X_cal.shape, X_va.shape, X_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a63988-d1f3-49cb-845a-3c9d76b335aa",
   "metadata": {
    "language": "python",
    "name": "Convert_Number_Dtype"
   },
   "outputs": [],
   "source": [
    "# Convert feature datasets\n",
    "X_tr = X_tr.astype('float32')\n",
    "X_cal = X_cal.astype('float32')\n",
    "X_va = X_va.astype('float32')\n",
    "X_te = X_te.astype('float32')\n",
    "\n",
    "# Convert label datasets\n",
    "y_tr = y_tr.astype('int8')\n",
    "y_cal = y_cal.astype('int8')\n",
    "y_va = y_va.astype('int8')\n",
    "y_te = y_te.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49280f-3e3d-4269-98ea-7f2f4cbbd960",
   "metadata": {
    "language": "python",
    "name": "Tuning_Functions"
   },
   "outputs": [],
   "source": [
    "def threshold_by_recall(y_true, y_proba, target_recall=0.30, min_precision=0.30):\n",
    "    \"\"\"\n",
    "    Choose threshold that reaches >= target_recall on validation.\n",
    "    Prefer the candidate with the highest precision; if none reach target,\n",
    "    pick the max-recall point (best effort).\n",
    "    \"\"\"\n",
    "    p, r, thr = precision_recall_curve(y_true, y_proba)\n",
    "    p, r = p[1:], r[1:]  # align with thresholds\n",
    "    idx = np.where(r >= target_recall)[0]\n",
    "    if len(idx) == 0:\n",
    "        best = int(np.argmax(r))\n",
    "        return float(thr[best]), float(p[best]), float(r[best])\n",
    "    ok = idx[p[idx] >= min_precision]\n",
    "    if len(ok) == 0:\n",
    "        ok = idx\n",
    "    best = ok[np.argmax(p[ok])]\n",
    "    return float(thr[best]), float(p[best]), float(r[best])\n",
    "\n",
    "def scan_threshold_grid(y_true, y_proba, recall_targets, precision_floors):\n",
    "    \"\"\"\n",
    "    Grid-search over (target_recall, min_precision) pairs on validation.\n",
    "    Returns a DataFrame with the chosen threshold and the achieved (precision, recall)\n",
    "    for each pair, plus a composite score (F_beta) you can use to select the 'best'.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for tr in recall_targets:\n",
    "        for mp in precision_floors:\n",
    "            thr, p_va, r_va = threshold_by_recall(y_true, y_proba, tr, mp)\n",
    "            beta = 2.0  # emphasize recall (F2)\n",
    "            if (p_va + r_va) == 0:\n",
    "                fbeta = 0.0\n",
    "            else:\n",
    "                fbeta = (1+beta**2) * (p_va*r_va) / (beta**2*p_va + r_va)\n",
    "            rows.append({\n",
    "                \"target_recall\": tr,\n",
    "                \"min_precision\": mp,\n",
    "                \"threshold\": thr,\n",
    "                \"precision_val\": p_va,\n",
    "                \"recall_val\": r_va,\n",
    "                \"f2_val\": fbeta\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"f2_val\",\"recall_val\",\"precision_val\"], ascending=False)\n",
    "\n",
    "def pick_best_by_recall(models_probas, y_true, recall_target=0.30, min_precision=0.30):\n",
    "    \"\"\"\n",
    "    models_probas: dict name -> (proba_va, proba_te)\n",
    "    Returns: dict of metrics keyed by model name using recall-first thresholding\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for name, (proba_va, proba_te) in models_probas.items():\n",
    "        thr, p_va, r_va = threshold_by_recall(y_true, proba_va, recall_target, min_precision)\n",
    "        out[name] = (thr, p_va, r_va, proba_te)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223df60a-b40f-4b75-a045-81e6bc6b0fc0",
   "metadata": {
    "language": "python",
    "name": "Parameter_Setup"
   },
   "outputs": [],
   "source": [
    "models = {} # will fill with: name -> (calibrated_model, X_va_matrix, X_te_matrix)\n",
    "ap_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "# --- subsample 10% for tuning ---\n",
    "sample_frac = 0.10\n",
    "X_tune = X_tr.sample(frac=sample_frac, random_state=42)\n",
    "y_tune = y_tr.loc[X_tune.index]\n",
    "print(f\"Tuning on {len(X_tune):,} rows ({sample_frac*100:.0f}%) of training data\")\n",
    "\n",
    "pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum()\n",
    "print(f\"Class imbalance ratio ≈ {pos_weight:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bd093-fd5a-4532-a6a7-d7e5fb729dfc",
   "metadata": {
    "language": "python",
    "name": "XGboost"
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    tree_method=\"hist\", n_jobs=-1, eval_metric=\"aucpr\",\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "grid_xgb = {\n",
    "    \"n_estimators\": [400],\n",
    "    \"learning_rate\": [0.05],\n",
    "    \"max_depth\": [6, 8],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.8],\n",
    "    \"min_child_weight\": [1, 3],\n",
    "    \"scale_pos_weight\": [pos_weight]\n",
    "}\n",
    "gs_xgb = GridSearchCV(xgb, grid_xgb, scoring=ap_scorer, cv=2, n_jobs=1, verbose=1)\n",
    "gs_xgb.fit(X_tune, y_tune)\n",
    "best_xgb = gs_xgb.best_estimator_\n",
    "best_xgb.fit(X_tr, y_tr)\n",
    "cal_xgb = CalibratedClassifierCV(best_xgb, cv=\"prefit\", method=\"sigmoid\")\n",
    "cal_xgb.fit(X_cal, y_cal)\n",
    "models[\"XGBoost\"] = (cal_xgb, X_va, X_te)\n",
    "print(\"XGBoost best:\", gs_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c687d-70a7-4d04-abc9-f14eac570d0f",
   "metadata": {
    "language": "python",
    "name": "CatBoost"
   },
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier(loss_function=\"Logloss\", eval_metric=\"AUC\",\n",
    "                         random_seed=RANDOM_STATE, verbose=False, thread_count=1)\n",
    "grid_cat = {\n",
    "    \"iterations\": [1200],\n",
    "    \"learning_rate\": [0.05],\n",
    "    \"depth\": [6, 8],\n",
    "    \"l2_leaf_reg\": [3.0],\n",
    "    \"class_weights\": [[1.0, pos_weight]]\n",
    "}\n",
    "gs_cat = GridSearchCV(cat, grid_cat, scoring=ap_scorer, cv=2, n_jobs=1, verbose=1)\n",
    "gs_cat.fit(X_tune, y_tune)\n",
    "best_cat = gs_cat.best_estimator_\n",
    "best_cat.fit(X_tr, y_tr)\n",
    "cal_cat = CalibratedClassifierCV(best_cat, cv=\"prefit\", method=\"sigmoid\")\n",
    "cal_cat.fit(X_cal, y_cal)\n",
    "models[\"CatBoost\"] = (cal_cat, X_va, X_te)\n",
    "print(\"CatBoost best:\", gs_cat.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d9036-3985-4171-bd48-de3d5858e9f0",
   "metadata": {
    "language": "python",
    "name": "Probability"
   },
   "outputs": [],
   "source": [
    "# Cobmpute and cache probabilities for every model\n",
    "probas = {}  # name -> dict with proba_va, proba_te, references to X sets\n",
    "for name, (model, Xv, Xt) in models.items():\n",
    "    proba_va = model.predict_proba(Xv)[:, 1]\n",
    "    proba_te = model.predict_proba(Xt)[:, 1]\n",
    "    probas[name] = {\"proba_va\": proba_va, \"proba_te\": proba_te, \"Xv\": Xv, \"Xt\": Xt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf81559-6056-4302-bf66-6b50b427f263",
   "metadata": {
    "language": "python",
    "name": "Tuning_Recall"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "recall_grid = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "precision_grid = [0.20, 0.30, 0.40, 0.50, 0.60, 0.70]\n",
    "max_diff = 0.05  # allowed difference between val and test metrics\n",
    "\n",
    "summary_rows = []\n",
    "chosen_ops = {}\n",
    "\n",
    "for name, d in probas.items():\n",
    "    df_scan = scan_threshold_grid(y_va, d[\"proba_va\"], recall_grid, precision_grid)\n",
    "    # Sort by F2 (best first), tie-break by recall, then precision\n",
    "    df_scan = df_scan.sort_values([\"f2_val\", \"recall_val\", \"precision_val\"], ascending=[False, False, False]).reset_index(drop=True)\n",
    "\n",
    "    found_valid = False\n",
    "\n",
    "    # Try each candidate threshold in order\n",
    "    for _, row in df_scan.iterrows():\n",
    "        thr = float(row[\"threshold\"])\n",
    "        y_pred_te = (d[\"proba_te\"] >= thr).astype(int)\n",
    "        tp = int(((y_te == 1) & (y_pred_te == 1)).sum())\n",
    "        fp = int(((y_te == 0) & (y_pred_te == 1)).sum())\n",
    "        prec_test = tp / max(tp + fp, 1)\n",
    "        rec_test  = tp / max(int(y_te.sum()), 1)\n",
    "\n",
    "        diff_prec = abs(float(row[\"precision_val\"]) - prec_test)\n",
    "        diff_rec  = abs(float(row[\"recall_val\"]) - rec_test)\n",
    "        overfit_flag = (diff_prec > max_diff) or (diff_rec > max_diff)\n",
    "\n",
    "        if not overfit_flag:\n",
    "            # ✅ Found a non-overfitting threshold\n",
    "            chosen_ops[name] = {\n",
    "                \"thr\": thr,\n",
    "                \"precision_val\": float(row[\"precision_val\"]),\n",
    "                \"recall_val\": float(row[\"recall_val\"]),\n",
    "                \"target_recall\": float(row[\"target_recall\"]),\n",
    "                \"min_precision\": float(row[\"min_precision\"]),\n",
    "                \"f2_val\": float(row[\"f2_val\"])\n",
    "            }\n",
    "            thr_for_summary = thr\n",
    "            note = f\"✅ Selected (no overfit, diff≤{max_diff})\"\n",
    "            found_valid = True\n",
    "            break\n",
    "\n",
    "    if not found_valid:\n",
    "        # ❌ No valid threshold found for this model\n",
    "        thr_for_summary = None\n",
    "        note = f\"⚠️ All thresholds overfit (val/test diff > {max_diff})\"\n",
    "        chosen_ops[name] = {\n",
    "            \"thr\": None,\n",
    "            \"precision_val\": None,\n",
    "            \"recall_val\": None,\n",
    "            \"target_recall\": None,\n",
    "            \"min_precision\": None,\n",
    "            \"f2_val\": None\n",
    "        }\n",
    "\n",
    "    # For summary reporting\n",
    "    if found_valid:\n",
    "        best_row = row\n",
    "        prec_test_final = prec_test\n",
    "        rec_test_final = rec_test\n",
    "        diff_prec_final = diff_prec\n",
    "        diff_rec_final = diff_rec\n",
    "        overfit_flag_final = False\n",
    "    else:\n",
    "        best_row = df_scan.iloc[0]  # fallback for reporting\n",
    "        y_pred_te = (d[\"proba_te\"] >= float(best_row[\"threshold\"])).astype(int)\n",
    "        tp = int(((y_te == 1) & (y_pred_te == 1)).sum())\n",
    "        fp = int(((y_te == 0) & (y_pred_te == 1)).sum())\n",
    "        prec_test_final = tp / max(tp + fp, 1)\n",
    "        rec_test_final  = tp / max(int(y_te.sum()), 1)\n",
    "        diff_prec_final = abs(float(best_row[\"precision_val\"]) - prec_test_final)\n",
    "        diff_rec_final = abs(float(best_row[\"recall_val\"]) - rec_test_final)\n",
    "        overfit_flag_final = True\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"model\": name,\n",
    "        \"thr\": thr_for_summary,\n",
    "        \"policy_val\": f\"rec≥{float(best_row['target_recall']):.2f} & prec≥{float(best_row['min_precision']):.2f}\",\n",
    "        \"precision_val@thr\": float(best_row[\"precision_val\"]),\n",
    "        \"recall_val@thr\": float(best_row[\"recall_val\"]),\n",
    "        \"f2_val\": float(best_row[\"f2_val\"]),\n",
    "        \"pr_auc_test\": average_precision_score(y_te, d[\"proba_te\"]),\n",
    "        \"roc_auc_test\": roc_auc_score(y_te, d[\"proba_te\"]),\n",
    "        \"precision_test@thr\": prec_test_final,\n",
    "        \"recall_test@thr\": rec_test_final,\n",
    "        \"alerts_test\": int((d[\"proba_te\"] >= (thr_for_summary or 0)).sum()),\n",
    "        \"positives_test\": int(y_te.sum()),\n",
    "        \"overfitting\": overfit_flag_final,\n",
    "        \"diff_prec\": diff_prec_final,\n",
    "        \"diff_rec\": diff_rec_final,\n",
    "        \"note\": note\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628e8ba-4a97-439a-a553-0a8147c16caf",
   "metadata": {
    "language": "python",
    "name": "Results"
   },
   "outputs": [],
   "source": [
    "# === Build and filter summary ===\n",
    "results_df = pd.DataFrame(summary_rows).sort_values(\n",
    "    [\"pr_auc_test\", \"recall_test@thr\", \"precision_test@thr\"],\n",
    "    ascending=[False, False, False]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6988bf-66d7-4c4e-8cb5-ad746eddd0a2",
   "metadata": {
    "language": "python",
    "name": "Results_2"
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f2e40-8117-4f9e-b189-5c5975dc1ff5",
   "metadata": {
    "language": "python",
    "name": "Valid_Results"
   },
   "outputs": [],
   "source": [
    "# Keep only non-overfitting (valid) models\n",
    "valid_results_df = results_df[results_df[\"overfitting\"] == False].copy().sort_values(\n",
    "    [\"pr_auc_test\", \"recall_test@thr\", \"precision_test@thr\"],\n",
    "    ascending=[False, False, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "if valid_results_df.empty:\n",
    "    print(\"No valid models found — all thresholds overfit.\")\n",
    "    best_model = None\n",
    "else:\n",
    "    best_model = valid_results_df.iloc[0].to_dict()\n",
    "    print(f\"✅ Best valid model: {best_model['model']} (thr={best_model['thr']})\")\n",
    "\n",
    "best_name = valid_results_df.iloc[0][\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ff3cd-9166-49c3-a383-fbad147a182f",
   "metadata": {
    "language": "python",
    "name": "Best_Model_Metrics"
   },
   "outputs": [],
   "source": [
    "valid_results_df[['model', 'thr', 'recall_test@thr', 'pr_auc_test', 'precision_test@thr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5513c98-9915-476a-9fba-877aff0cfd98",
   "metadata": {
    "language": "python",
    "name": "Confusion_Matrix"
   },
   "outputs": [],
   "source": [
    "best_thr = chosen_ops[best_name][\"thr\"]\n",
    "best_model, Xv_best, Xt_best = models[best_name]\n",
    "\n",
    "y_proba_best_te = probas[best_name][\"proba_te\"]\n",
    "y_pred_best = (y_proba_best_te >= best_thr).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_te, y_pred_best)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "labels = np.array([[\"TN\", \"FP\"], [\"FN\", \"TP\"]])\n",
    "\n",
    "# Compact confusion matrix plot\n",
    "fig, ax = plt.subplots(figsize=(3.2, 3))  # reduced from (5.5, 5)\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=\"Blues\")\n",
    "\n",
    "# Label axes\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels([\"No Outage\", \"Outage\"], fontsize=6)\n",
    "ax.set_yticklabels([\"No Outage\", \"Outage\"], fontsize=6)\n",
    "ax.set_xlabel(\"Predicted\", fontsize=6)\n",
    "ax.set_ylabel(\"True\", fontsize=6)\n",
    "\n",
    "# Compact title\n",
    "ax.set_title(\n",
    "    f\"Confusion Matrix — {best_name}\\n\"\n",
    "    f\"thr={best_thr:.3f} | rec≥{chosen_ops[best_name]['target_recall']:.2f}, \"\n",
    "    f\"prec≥{chosen_ops[best_name]['min_precision']:.2f}\",\n",
    "    fontsize=7, pad=6\n",
    ")\n",
    "\n",
    "# Annotate cells with smaller text and color contrast\n",
    "for (i, j), value in np.ndenumerate(cm):\n",
    "    color = \"white\" if value > cm.max() / 2 else \"black\"\n",
    "    ax.text(j, i, f\"{labels[i,j]}\\n{value:,}\",\n",
    "            ha='center', va='center', fontsize=6, fontweight='bold', color=color)\n",
    "\n",
    "# Smaller colorbar for compact layout\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.04, pad=0.02)\n",
    "cbar.ax.tick_params(labelsize=5)\n",
    "\n",
    "plt.tight_layout(pad=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Compute and print summary metrics\n",
    "prec = tp / max(tp + fp, 1)\n",
    "rec  = tp / max(tp + fn, 1)\n",
    "f1   = (2 * prec * rec) / max(prec + rec, 1e-12)\n",
    "\n",
    "print(f\"TN={tn:,}  FP={fp:,}  FN={fn:,}  TP={tp:,}\")\n",
    "print(f\"Precision={prec:.4f}  Recall={rec:.4f}  F1={f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272499b1-5250-4f66-99fe-c4d596d42998",
   "metadata": {
    "language": "python",
    "name": "PR_Curve"
   },
   "outputs": [],
   "source": [
    "proba_te_best = best_model.predict_proba(Xt_best)[:, 1]\n",
    "\n",
    "# Compute precision–recall curve\n",
    "p, r, thr = precision_recall_curve(y_te, proba_te_best)\n",
    "ap = average_precision_score(y_te, proba_te_best)\n",
    "\n",
    "# Find the precision and recall corresponding to the chosen threshold\n",
    "# (Note: thresholds returned by precision_recall_curve are for proba >= thr)\n",
    "idx = np.argmin(np.abs(thr - best_thr))  # closest threshold\n",
    "best_p, best_r = p[idx], r[idx]\n",
    "\n",
    "# Compact Precision–Recall Curve\n",
    "plt.figure(figsize=(3.5, 3))  # reduced from (7, 6)\n",
    "plt.plot(r, p, label=f\"{best_name} (AP={ap:.3f})\", lw=1.2)\n",
    "plt.scatter(best_r, best_p, color=\"red\", s=30, zorder=5, label=f\"Best thr={best_thr:.2f}\")\n",
    "\n",
    "# Labels and title (smaller fonts)\n",
    "plt.xlabel(\"Recall\", fontsize=7)\n",
    "plt.ylabel(\"Precision\", fontsize=7)\n",
    "plt.title(\"Precision–Recall Curve (Test)\", fontsize=8, pad=4)\n",
    "\n",
    "# Smaller tick labels\n",
    "plt.tick_params(axis='both', which='major', labelsize=6)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=5)\n",
    "\n",
    "# Legend and grid\n",
    "plt.legend(fontsize=6, loc=\"upper right\", frameon=False)\n",
    "plt.grid(True, linestyle='--', alpha=0.5, linewidth=0.6)\n",
    "\n",
    "plt.tight_layout(pad=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dd2d8-e35e-4f5f-8c63-2d9299d6a6ff",
   "metadata": {
    "language": "python",
    "name": "Feature_Importance"
   },
   "outputs": [],
   "source": [
    "if best_name == \"Logistic\":\n",
    "    # Xt_best is a scaled array; wrap with feature_cols_all names\n",
    "    Xt_best_df = pd.DataFrame(Xt_best, columns=feature_cols_all)\n",
    "else:\n",
    "    # Xt_best is already a DataFrame with full feature columns\n",
    "    Xt_best_df = Xt_best.copy()\n",
    "\n",
    "X_pi = Xt_best_df.copy()\n",
    "y_pi = y_te.copy()\n",
    "\n",
    "perm = permutation_importance(\n",
    "    best_model, X_pi, y_pi,\n",
    "    n_repeats=3, random_state=RANDOM_STATE, n_jobs=1,\n",
    "    scoring=\"average_precision\"\n",
    ")\n",
    "\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": X_pi.columns,\n",
    "    \"importance_mean\": perm.importances_mean,\n",
    "    \"importance_std\": perm.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "print(f\"Best model: {best_name}\")\n",
    "print(imp.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5891ec-950f-4036-807d-c18a0f475cf3",
   "metadata": {
    "language": "python",
    "name": "Feature_Importance_Plot"
   },
   "outputs": [],
   "source": [
    "# Select top 10 features\n",
    "imp_top10 = imp.head(10)\n",
    "\n",
    "# Horizontal bar plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.barh(imp_top10[\"feature\"], imp_top10[\"importance_mean\"], xerr=imp_top10[\"importance_std\"], color='skyblue')\n",
    "plt.xlabel(\"Permutation Importance (Mean ± Std)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Top 10 Permutation Feature Importances - {best_name} Model\")\n",
    "plt.gca().invert_yaxis()  # largest importance on top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeb11e-f7b9-4554-937b-6681b6c6e0da",
   "metadata": {
    "language": "python",
    "name": "Save_Model"
   },
   "outputs": [],
   "source": [
    "registry = Registry(session=session)\n",
    "\n",
    "# pick a small sample to avoid uploading entire dataset\n",
    "sample_input = pd.DataFrame(Xv_best).head(50)\n",
    "\n",
    "best_name = best_name.lower()\n",
    "\n",
    "# Ensure valid dependency mapping\n",
    "dep_name = \"catboost\" if \"catboost\" in best_name else best_name\n",
    "\n",
    "model_version = registry.log_model(\n",
    "    model=best_model,\n",
    "    model_name=f\"{best_name}_outage_predictor\",\n",
    "    version_name=\"v1\",\n",
    "    conda_dependencies=[\"scikit-learn\", dep_name],\n",
    "    sample_input_data=sample_input\n",
    ")\n",
    "\n",
    "print(\"✅ Model registered!\")\n",
    "print(f\"Model Name: {model_version.model_name}\")\n",
    "print(f\"Version: {model_version.version_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ba033-d21d-4306-8d0d-b1691c884101",
   "metadata": {
    "language": "python",
    "name": "Load_n_Inference"
   },
   "outputs": [],
   "source": [
    "# Get the model first\n",
    "model = registry.get_model(f\"{best_name}_outage_predictor\")\n",
    "\n",
    "# Get the specific version from the model object\n",
    "model_version = model.version(\"v1\")\n",
    "\n",
    "# Load the actual Python model artifact\n",
    "loaded_model = model_version.load()\n",
    "\n",
    "# Run inference\n",
    "y_pred_new = loaded_model.predict(Xt_best)\n",
    "y_pred_new[:5]"
   ]
  }
 ]
}